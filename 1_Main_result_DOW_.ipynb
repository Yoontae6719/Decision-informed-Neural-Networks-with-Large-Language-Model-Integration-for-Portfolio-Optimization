{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c50b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from models import DBLM2\n",
    "from data_provider.data_loader import Dataset_Pred, get_result_list2\n",
    "from utils.losses import (\n",
    "    cvx_layer, cpu_compute_dfl_loss\n",
    ")\n",
    "from utils.infer_tool import (\n",
    "    group_data_by_month_new, load_data, calculate_metrics, plot_cumulative_return\n",
    ")\n",
    "from utils.tools import del_files, EarlyStopping, adjust_learning_rate, vali\n",
    "from utils.main_analysis_DOW import *\n",
    "# Matplotlib settings\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "# Torch and environment settings\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "# Set random seed\n",
    "fix_seed = 2024\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "\n",
    "\n",
    "def update_args(args):\n",
    "    \"\"\"\n",
    "    Updates the args object with values from the shell script.\n",
    "    All updates are based on the provided shell script variables.\n",
    "    \"\"\"\n",
    "    shell_vars = {\n",
    "        'HF_HOME': '/home/work/DBLM/huggingface_cache',\n",
    "        'model_name': 'DBLM2',\n",
    "        'train_epochs': 1,\n",
    "        'learning_rate': 0.01,\n",
    "        'llama_layers': 32,\n",
    "        'master_port': 97,  # master_port in shell script is 00097, updated to integer 97\n",
    "        'num_process': 4,\n",
    "        'batch_size': 16,\n",
    "        'comment': 'DBLM-DOW'\n",
    "    }\n",
    "\n",
    "    # Update environment variables if needed\n",
    "    os.environ['HF_HOME'] = shell_vars['HF_HOME']\n",
    "\n",
    "    # Update args with shell script variables\n",
    "    args.model = shell_vars['model_name']\n",
    "    args.train_epochs = shell_vars['train_epochs']\n",
    "    args.learning_rate = shell_vars['learning_rate']\n",
    "    args.llm_layers = shell_vars['llama_layers']\n",
    "    args.num_workers = shell_vars['num_process']\n",
    "    args.batch_size = shell_vars['batch_size']\n",
    "    args.des = 'Exp'  # As per shell script\n",
    "    args.model_id = 'DOW'       # Updated from shell script\n",
    "    args.data_path = 'DOW30_ret.csv'  # Updated from shell script\n",
    "    args.seq_len = 252  # As per shell script\n",
    "    args.label_len = 0  # As per shell script\n",
    "    args.pred_len = 22  # As per shell script\n",
    "    args.d_ff = 36\n",
    "    args.num_hidden = 512\n",
    "    args.enc_in = 30  # As per shell script\n",
    "    args.dec_in = 30  # As per shell script\n",
    "    args.loss_alpha = 0.4\n",
    "    args.lambda_value = \"B\"\n",
    "    args.num_heads = 4\n",
    "    args.num_enc_l = 1\n",
    "    args.comment = shell_vars['comment']\n",
    "    return args\n",
    "parser = argparse.ArgumentParser(description='DBLM')\n",
    "\n",
    "# Basic config\n",
    "parser.add_argument('--task_name', type=str , default='Forecasting', help='task name, options:[DFL, Forecasting]')\n",
    "parser.add_argument('--is_training', type=int, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, default='test', help='model id')\n",
    "parser.add_argument('--model_comment', type=str,  default='none', help='prefix when saving test results')\n",
    "parser.add_argument('--model', type=str, default='Autoformer', help='model name, options: [Autoformer, DLinear]')\n",
    "parser.add_argument('--seed', type=int, default=2024, help='random seed')\n",
    "\n",
    "# Data loader\n",
    "parser.add_argument('--data', type=str, default='DOW30', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./preprocessing', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='DOW30_ret.csv', help='data file')\n",
    "parser.add_argument('--loader', type=str, default='modal', help='dataset type')  # CHECK\n",
    "parser.add_argument('--freq', type=str, default='d',\n",
    "                    help='freq for time features encoding, '\n",
    "                         'options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], '\n",
    "                         'you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "parser.add_argument('--is_price', type=bool, default=False, help='Use price data?')\n",
    "\n",
    "# Forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=252, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=22, help='prediction sequence length')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; '\n",
    "                         'M:multivariate predict multivariate, S: univariate predict univariate, '\n",
    "                         'MS:multivariate predict univariate')\n",
    "# View define\n",
    "parser.add_argument('--num_heads', type=int, default=4, help='number of heads')\n",
    "parser.add_argument('--num_enc_l', type=int, default=1, help='number of encoder layers')\n",
    "parser.add_argument('--loss_alpha', type=float, default=0.4, help='Loss function alpha')\n",
    "\n",
    "# Model define\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--enc_in', type=int, default=30, help='encoder input size')\n",
    "parser.add_argument('--dec_in', type=int, default=30, help='decoder input size')\n",
    "parser.add_argument('--num_hidden', type=int, default=256, help='number of hidden units')\n",
    "parser.add_argument('--embed', type=str, default='timeF', help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in encoder')\n",
    "parser.add_argument('--llm_model', type=str, default='GPT2', help='LLM model')  # LLAMA, GPT2, BERT\n",
    "parser.add_argument('--llm_dim', type=int, default=768, help='LLM model dimension')  # LLama7b:4096; GPT2-small:768; BERT-base:768\n",
    "parser.add_argument('--lambda_value', type=str, default='B', help='Lambda e.g.(\"HA\", \"A\", \"B\", \"C\", \"HC\")')\n",
    "\n",
    "# Prompt\n",
    "parser.add_argument('--ticker_dict_path', type=str, default='./view_bank/ticker_dict_dow.json', help='Path to the ticker dictionary JSON file')\n",
    "parser.add_argument('--sector_dict_path', type=str, default='./view_bank/sector_dict_dow.json', help='Path to the ticker dictionary JSON file')\n",
    "\n",
    "parser.add_argument('--triplet_loss_weight', type=float, default=1.0, help='Weight for the triplet loss')\n",
    "parser.add_argument('--d_ff', type=int, default=64, help='Dimension of feedforward network')\n",
    "\n",
    "# Optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=1, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--align_epochs', type=int, default=10, help='alignment epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='batch size of train input data')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=8, help='batch size of model evaluation')\n",
    "parser.add_argument('--patience', type=int, default=10, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='MSE', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')  # type1 COS\n",
    "parser.add_argument('--pct_start', type=float, default=0.2, help='pct_start')  # Check\n",
    "parser.add_argument('--use_amp', action='store_true', default=False, help='use automatic mixed precision training')\n",
    "parser.add_argument('--llm_layers', type=int, default=6)\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# Update args with shell script variables\n",
    "args = update_args(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ef8b5",
   "metadata": {},
   "source": [
    "### ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a957b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/work/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/work/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/work/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/work/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/work/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "/home/work/DINN_COPY/utils/main_analysis_DOW.py:156: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_returns = df['Cumulative Value'].resample('M').last().pct_change().dropna()\n",
      "/home/work/DINN_COPY/utils/main_analysis_DOW.py:157: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_rf = (1 + df['rf']).resample('M').prod() - 1\n",
      "/home/work/DINN_COPY/utils/main_analysis_DOW.py:156: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_returns = df['Cumulative Value'].resample('M').last().pct_change().dropna()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Metric  High aggressive  Aggressive   Balanced  \\\n",
      "0   Annualized Return (No RF)         0.582070    0.566471   0.552191   \n",
      "1      Annualized Std (No RF)         0.484307    0.471611   0.468939   \n",
      "2    Annualized Excess Return         0.564508    0.549081   0.534959   \n",
      "3       Annualized Excess Std         0.484228    0.471522   0.468846   \n",
      "4                Sharpe Ratio         1.178642    1.177293   1.153552   \n",
      "5               Sortino Ratio         1.798438    1.768547   1.724511   \n",
      "6           Arithmetic Return         0.459152    0.449225   0.440051   \n",
      "7            Geometric Return         0.402013    0.396386   0.385350   \n",
      "8            Maximum Drawdown         0.588314    0.564196   0.562952   \n",
      "9         Annualized Skewness         0.537524    0.505643   0.503438   \n",
      "10        Annualized Kurtosis         9.749883   10.332147  10.510344   \n",
      "11          Cumulative Return         2.848232    2.786999   2.669039   \n",
      "12            Monthly 95% VaR         0.157055    0.157604   0.157480   \n",
      "13                     Wealth         3.923627    3.861201   3.740923   \n",
      "14            Return Over VaR         0.227597    0.221309   0.216981   \n",
      "\n",
      "    Conservative  High conservative        EWP  \n",
      "0       0.499805           0.428502   0.146985  \n",
      "1       0.452976           0.434129   0.199569  \n",
      "2       0.483153           0.412638   0.134236  \n",
      "3       0.452884           0.434040   0.199521  \n",
      "4       1.078557           0.961135   0.680162  \n",
      "5       1.582954           1.375590   0.829423  \n",
      "6       0.405662           0.356879   0.137174  \n",
      "7       0.347932           0.294177   0.121659  \n",
      "8       0.551348           0.539426   0.299125  \n",
      "9       0.330090           0.178564  -0.337061  \n",
      "10      9.283565           9.063888  18.918629  \n",
      "11      2.289489           1.796671   0.580703  \n",
      "12      0.134158           0.130820   0.073920  \n",
      "13      3.353937           2.851464   1.595706  \n",
      "14      0.229062           0.203141   0.133067  \n",
      "Results processed for loss=4, head=2, num_enc_l=1, dff=12, num_hidden=256, ii=0, risk=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/DINN_COPY/utils/main_analysis_DOW.py:157: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_rf = (1 + df['rf']).resample('M').prod() - 1\n",
      "/home/work/DINN_COPY/utils/main_analysis_DOW.py:156: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_returns = df['Cumulative Value'].resample('M').last().pct_change().dropna()\n",
      "/home/work/DINN_COPY/utils/main_analysis_DOW.py:157: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_rf = (1 + df['rf']).resample('M').prod() - 1\n",
      "/home/work/DINN_COPY/utils/main_analysis_DOW.py:156: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_returns = df['Cumulative Value'].resample('M').last().pct_change().dropna()\n",
      "/home/work/DINN_COPY/utils/main_analysis_DOW.py:157: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_rf = (1 + df['rf']).resample('M').prod() - 1\n",
      "/home/work/DINN_COPY/utils/main_analysis_DOW.py:156: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_returns = df['Cumulative Value'].resample('M').last().pct_change().dropna()\n",
      "/home/work/DINN_COPY/utils/main_analysis_DOW.py:157: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_rf = (1 + df['rf']).resample('M').prod() - 1\n",
      "/home/work/DINN_COPY/utils/main_analysis_DOW.py:156: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_returns = df['Cumulative Value'].resample('M').last().pct_change().dropna()\n",
      "/home/work/DINN_COPY/utils/main_analysis_DOW.py:157: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_rf = (1 + df['rf']).resample('M').prod() - 1\n"
     ]
    }
   ],
   "source": [
    "noise_values = [0.0543, 0.5690, 0.9545, 2.4305, 3.4623]\n",
    "\n",
    "for risk in [\"B\"]: #:    \n",
    "\n",
    "    # 실험 실행\n",
    "    for loss in [4]:\n",
    "        for head in [2]:\n",
    "            for encoder in [1]:\n",
    "                for dff in [12]:\n",
    "                    for hidden in [256]:\n",
    "                        for ii in [0]:\n",
    "                            try:\n",
    "                                model_name_template = f\"mdDBLM2_daDOW30_sq252_pr22_lo{loss}_he{head}_en{encoder}_llGPT2_la{risk}_deExp_is16_prFalse_dff{dff}_hi{hidden}_ii{ii}_Noise_None2-DOW\"\n",
    "                                \n",
    "                                weight_lists = run_experiment(args, loss, head, encoder, dff, hidden, noise_values, model_name_template, risk,ii)\n",
    "                                try:\n",
    "                                    process_results(weight_lists, f\"loss={loss}, head={head}, num_enc_l={encoder}, dff={dff}, num_hidden={hidden}, ii={ii}\", loss, head, encoder, dff, hidden, risk, ii)\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error processing results for loss={loss}, head={head}, num_enc_l={encoder}, dff={dff}, num_hidden={hidden}, risk={risk}, ii={ii}: {e}\")\n",
    "                                    pass\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error running experiment for loss={loss}, head={head}, num_enc_l={encoder}, dff={dff}, num_hidden={hidden}, risk={risk}, ii={ii}: {e}\")\n",
    "                                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dbb50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.1 (NGC 23.09/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
